Fine-Tuning DistilBERT with LoRA for Text Classification

This project investigates the application of Low-Rank Adaptation (LoRA) for fine-tuning a pre-trained DistilBERT model on text classification tasks. LoRA aims to reduce trainable parameters, improving efficiency for large language models like DistilBERT.

Goals

Implement LoRA fine-tuning for DistilBERT on text classification.
Evaluate LoRA's impact on performance and parameter reduction.
Explore LoRA hyperparameter effects on the model.

RESULTS
Untrained Model's Performance, here are the results
<img width="550" alt="Screenshot 2024-06-10 at 12 44 46 AM" src="https://github.com/codereyinish/Fine_Tuning_With_LoRA/assets/86160294/dabdbad1-2e2d-4557-9cd4-8a1635c8b9ef">
<br>
Fine_tuned Model's Performance:
<img width="453" alt="Screenshot 2024-06-10 at 12 26 03 AM" src="https://github.com/codereyinish/Fine_Tuning_With_LoRA/assets/86160294/c14d8a8a-a44c-4261-a5b2-cda4dca85666">



