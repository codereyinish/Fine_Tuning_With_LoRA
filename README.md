Fine-Tuning DistilBERT with LoRA for Text Classification

This project investigates the application of Low-Rank Adaptation (LoRA) for fine-tuning a pre-trained DistilBERT model on text classification tasks. LoRA aims to reduce trainable parameters, improving efficiency for large language models like DistilBERT.

Goals

Implement LoRA fine-tuning for DistilBERT on text classification.
Evaluate LoRA's impact on performance and parameter reduction.
Explore LoRA hyperparameter effects on the model.